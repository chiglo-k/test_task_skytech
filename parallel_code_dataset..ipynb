{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import concat, lpad, lit\n",
        "import pandas as pd\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import codecs\n",
        "import os\n",
        "\n",
        "def fix_kernelspec_metadata(file_path):\n",
        "    \"\"\"\n",
        "    Исправляет отсутствующие поля в kernelspec метаданных\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Создаем резервную копию\n",
        "        backup_path = file_path + '.backup'\n",
        "        if os.path.exists(file_path):\n",
        "            with open(file_path, 'rb') as src, open(backup_path, 'wb') as dst:\n",
        "                dst.write(src.read())\n",
        "            print(f\"Создана резервная копия: {backup_path}\")\n",
        "        \n",
        "        # Читаем ноутбук\n",
        "        with codecs.open(file_path, 'r', encoding='utf-8-sig') as f:\n",
        "            notebook_data = json.load(f)\n",
        "        \n",
        "        # Проверяем и исправляем kernelspec\n",
        "        if 'metadata' not in notebook_data:\n",
        "            notebook_data['metadata'] = {}\n",
        "        \n",
        "        if 'kernelspec' not in notebook_data['metadata']:\n",
        "            notebook_data['metadata']['kernelspec'] = {}\n",
        "        \n",
        "        kernelspec = notebook_data['metadata']['kernelspec']\n",
        "        \n",
        "        # Определяем display_name на основе существующих данных\n",
        "        if 'name' in kernelspec:\n",
        "            name = kernelspec['name']\n",
        "            language = kernelspec.get('language', 'unknown')\n",
        "            \n",
        "            # Словарь соответствий для популярных ядер\n",
        "            display_names = {\n",
        "                'spark2-scala': 'Apache Toree - Scala',\n",
        "                'spark2-python': 'Apache Toree - Python',\n",
        "                'spark2-r': 'Apache Toree - R',\n",
        "                'python3': 'Python 3',\n",
        "                'python2': 'Python 2',\n",
        "                'scala': 'Scala',\n",
        "                'ir': 'R',\n",
        "                'julia': 'Julia'\n",
        "            }\n",
        "            \n",
        "            # Устанавливаем display_name\n",
        "            if name in display_names:\n",
        "                kernelspec['display_name'] = display_names[name]\n",
        "            elif language == 'scala':\n",
        "                kernelspec['display_name'] = 'Scala'\n",
        "            elif language == 'python':\n",
        "                kernelspec['display_name'] = 'Python 3'\n",
        "            else:\n",
        "                kernelspec['display_name'] = f\"{language.title()} Kernel\"\n",
        "        else:\n",
        "            # Если name отсутствует, устанавливаем значения по умолчанию\n",
        "            kernelspec['name'] = 'python3'\n",
        "            kernelspec['display_name'] = 'Python 3'\n",
        "            kernelspec['language'] = 'python'\n",
        "        \n",
        "        # Убеждаемся, что все обязательные поля присутствуют\n",
        "        if 'name' not in kernelspec:\n",
        "            kernelspec['name'] = 'python3'\n",
        "        if 'language' not in kernelspec:\n",
        "            kernelspec['language'] = 'python'\n",
        "        \n",
        "        # Проверяем структуру ноутбука\n",
        "        if 'nbformat' not in notebook_data:\n",
        "            notebook_data['nbformat'] = 4\n",
        "        if 'nbformat_minor' not in notebook_data:\n",
        "            notebook_data['nbformat_minor'] = 4\n",
        "        if 'cells' not in notebook_data:\n",
        "            notebook_data['cells'] = []\n",
        "        \n",
        "        # Сохраняем исправленный ноутбук\n",
        "        with codecs.open(file_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(notebook_data, f, indent=2, ensure_ascii=False)\n",
        "        \n",
        "        print(f\"Kernelspec успешно исправлен:\")\n",
        "        print(f\"  name: {kernelspec['name']}\")\n",
        "        print(f\"  display_name: {kernelspec['display_name']}\")\n",
        "        print(f\"  language: {kernelspec['language']}\")\n",
        "        \n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при исправлении kernelspec: {e}\")\n",
        "        return False\n",
        "\n",
        "# Использование\n",
        "notebook_path = r\"C:\\Users\\Chigl\\Downloads\\parallel_code_dataset..ipynb\"\n",
        "fix_kernelspec_metadata(notebook_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "# Проверка кластера\n",
        "print(f\"Spark версия: {spark.version}\")\n",
        "print(f\"Доступно ядер: {spark.sparkContext.defaultParallelism}\")\n",
        "print(f\"Мастер URL: {spark.sparkContext.master}\")\n",
        "\n",
        "# Информация о кластере\n",
        "print(f\"Executor instances: {spark.conf.get('spark.executor.instances', 'auto')}\")\n",
        "print(f\"Executor memory: {spark.conf.get('spark.executor.memory', 'default')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
        "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
        "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
        "spark.conf.set(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\")\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", \"400\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "bucket_name = 'sparkwrk'\n",
        "\n",
        "df = spark.read.option(\"header\", True).csv(f's3a://{bucket_name}/bi_datasets/custom_1988_2020.csv')  # Датасет по трейдингу (Спасибо, Японскому правительству!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "initial_partitions = df.rdd.getNumPartitions()\n",
        "\n",
        "print(f\"Исходное количество партиций: {initial_partitions}\")\n",
        "\n",
        "print(\"Оригинальные колонки:\")\n",
        "all_columns = df.columns\n",
        "for i, col_name in enumerate(all_columns):\n",
        "    print(f\"   {i}: '{col_name}'\")\n",
        "\n",
        "print(f\"\\nВсего колонок: {len(all_columns)}\")\n",
        "\n",
        "rename_dict = {}\n",
        "for col_name in all_columns:\n",
        "    if \"198801\" in str(col_name) or col_name == \"198801\":\n",
        "        rename_dict[col_name] = \"ym\"\n",
        "    elif col_name == \"1\":\n",
        "        rename_dict[col_name] = \"exp_imp\"  \n",
        "    elif col_name == \"103\":\n",
        "        rename_dict[col_name] = \"hs9\"\n",
        "    elif col_name == \"100\":\n",
        "        rename_dict[col_name] = \"customs\"\n",
        "    elif \"000000190\" in str(col_name) or col_name == \"000000190\":\n",
        "        rename_dict[col_name] = \"country\"\n",
        "    elif col_name == \"0\":\n",
        "        rename_dict[col_name] = \"q1\"\n",
        "    elif col_name == \"35843\":\n",
        "        rename_dict[col_name] = \"q2\"  \n",
        "    elif col_name == \"34353\":\n",
        "        rename_dict[col_name] = \"value_yen\"\n",
        "\n",
        "print(f\"\\nСоответствия:\")\n",
        "\n",
        "for old_name, new_name in rename_dict.items():\n",
        "    print(f\"   '{old_name}' -> '{new_name}'\")\n",
        "\n",
        "df_processed = df.select([col(c).alias(rename_dict.get(c, c)) for c in df.columns])\n",
        "\n",
        "\n",
        "if initial_partitions * 2 > 200:\n",
        "    optimal_partitions = initial_partitions * 2\n",
        "else:\n",
        "    optimal_partitions = 200\n",
        "\n",
        "df_processed = df_processed.repartition(optimal_partitions)\n",
        "print(f\"Оптимизированное количество партиций: {df_processed.rdd.getNumPartitions()}\")\n",
        "\n",
        "# Приводим типы данных\n",
        "if \"ym\" in df_processed.columns:\n",
        "    df_processed = df_processed.withColumn(\"ym\", col(\"ym\").cast(IntegerType()))\n",
        "    df_processed = df_processed.withColumn(\"year\", (col(\"ym\") / 100).cast(IntegerType()))\n",
        "    df_processed = df_processed.withColumn(\"month\", when((col(\"ym\") % 100) < 10, concat(lit(\"0\"), (col(\"ym\") % 100).cast(StringType()))).otherwise((col(\"ym\") % 100).cast(StringType())))\n",
        "\n",
        "if \"exp_imp\" in df_processed.columns:\n",
        "    df_processed = df_processed.withColumn(\"exp_imp\", col(\"exp_imp\").cast(StringType()))\n",
        "\n",
        "if \"hs9\" in df_processed.columns:\n",
        "    df_processed = df_processed.withColumn(\"hs9\", col(\"hs9\").cast(StringType()))\n",
        "\n",
        "if \"customs\" in df_processed.columns:\n",
        "    df_processed = df_processed.withColumn(\"customs\", col(\"customs\").cast(IntegerType()))\n",
        "\n",
        "if \"country\" in df_processed.columns:\n",
        "    df_processed = df_processed.withColumn(\"country\", col(\"country\").cast(StringType()))\n",
        "\n",
        "if \"q1\" in df_processed.columns:\n",
        "    df_processed = df_processed.withColumn(\"q1\", col(\"q1\").cast(LongType()))\n",
        "\n",
        "if \"q2\" in df_processed.columns:\n",
        "    df_processed = df_processed.withColumn(\"q2\", col(\"q2\").cast(LongType()))\n",
        "\n",
        "if \"value_yen\" in df_processed.columns:\n",
        "    df_processed = df_processed.withColumn(\"value_yen\", col(\"value_yen\").cast(LongType()))\n",
        "\n",
        "df_processed.cache()\n",
        "\n",
        "print(\"Схема после обработки:\")\n",
        "df_processed.printSchema()\n",
        "\n",
        "print(\"Образец данных:\")\n",
        "available_cols = [col for col in [\"year\", \"month\", \"exp_imp\", \"hs9\", \"country\", \"q1\", \"q2\", \"value_yen\"] if col in df_processed.columns]\n",
        "df_processed.select(*available_cols).show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "\n",
        "filters = []\n",
        "\n",
        "if \"year\" in df_processed.columns and \"month\" in df_processed.columns:\n",
        "    filters.append(col(\"year\").isNotNull() & col(\"month\").isNotNull())\n",
        "\n",
        "if \"exp_imp\" in df_processed.columns:\n",
        "    filters.append(col(\"exp_imp\").isNotNull() & (col(\"exp_imp\") != \"\"))\n",
        "\n",
        "if \"hs9\" in df_processed.columns:\n",
        "    filters.append(col(\"hs9\").isNotNull() & (col(\"hs9\") != \"\"))\n",
        "\n",
        "if \"country\" in df_processed.columns:\n",
        "    filters.append(col(\"country\").isNotNull() & (col(\"country\") != \"\"))\n",
        "\n",
        "if \"value_yen\" in df_processed.columns:\n",
        "    filters.append(col(\"value_yen\").isNotNull() & (col(\"value_yen\") > 0))\n",
        "\n",
        "if \"q1\" in df_processed.columns:\n",
        "    filters.append(col(\"q1\").isNotNull())\n",
        "\n",
        "if \"q2\" in df_processed.columns:\n",
        "    filters.append(col(\"q2\").isNotNull())\n",
        "\n",
        "# Объединяем все фильтры\n",
        "combined_filter = filters[0]\n",
        "for filter_condition in filters[1:]:\n",
        "    combined_filter = combined_filter & filter_condition\n",
        "\n",
        "df_clean = df_processed.filter(combined_filter)\n",
        "print(f\"После удаления пустых: {df_clean.count():,} записей\")\n",
        "\n",
        "# Фильтруем строки с цифрами в hs9\n",
        "if \"hs9\" in df_clean.columns:\n",
        "    df_with_digits = df_clean.filter(col(\"hs9\").rlike(\".*[0-9].*\"))\n",
        "    print(f\"После фильтрации строк с цифрами: {df_with_digits.count():,} записей\")\n",
        "else:\n",
        "    df_with_digits = df_clean\n",
        "    print(\"Колонка hs9 не найдена\")\n",
        "\n",
        "# Поиск дублей\n",
        "key_fields = [field for field in [\"year\", \"month\", \"exp_imp\", \"hs9\", \"country\", \"customs\"] if field in df_with_digits.columns]\n",
        "print(f\"Ключевые поля для дедупликации: {key_fields}\")\n",
        "\n",
        "if len(key_fields) > 0:\n",
        "    if \"country\" in key_fields:\n",
        "        df_partitioned = df_with_digits.repartition(\"country\")\n",
        "        print(\"Данные перепартиционированы по country\")\n",
        "    else:\n",
        "        df_partitioned = df_with_digits\n",
        "    \n",
        "    print(\"Анализ дублей с параллелизмом...\")\n",
        "    duplicates = df_partitioned.groupBy(*key_fields).count().filter(col(\"count\") > 1).cache()\n",
        "    \n",
        "    duplicate_count = duplicates.count()\n",
        "    total_duplicate_records = duplicates.agg(sum(\"count\")).collect()[0][0] if duplicate_count > 0 else 0\n",
        "    \n",
        "    print(f\"Найдено групп дублей: {duplicate_count:,}\")\n",
        "    print(f\"Всего дублированных записей: {total_duplicate_records:,}\")\n",
        "    \n",
        "    if duplicate_count > 0:\n",
        "        print(\"Топ-10 дублей:\")\n",
        "        duplicates.orderBy(desc(\"count\")).show(10)\n",
        "    \n",
        "    df_deduplicated = df_partitioned.dropDuplicates(key_fields)\n",
        "else:\n",
        "    df_deduplicated = df_with_digits\n",
        "\n",
        "final_count = df_deduplicated.count()\n",
        "print(f\"Финальный датасет: {final_count:,} записей\")\n",
        "print(f\"Финальное количество партиций: {df_deduplicated.rdd.getNumPartitions()}\")\n",
        "\n",
        "df_deduplicated.cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "\n",
        "if \"year\" in df_deduplicated.columns and \"month\" in df_deduplicated.columns:\n",
        "    \n",
        "    df_time_partitioned = df_deduplicated.repartition(\"year\", \"month\")\n",
        "    print(f\"Данные перепартиционированы по времени: {df_time_partitioned.rdd.getNumPartitions()} партиций\")\n",
        "    \n",
        "    agg_expressions = []\n",
        "    \n",
        "    if \"exp_imp\" in df_deduplicated.columns:\n",
        "        agg_expressions.append(countDistinct(\"exp_imp\").alias(\"unique_exp_imp\"))\n",
        "    \n",
        "    if \"hs9\" in df_deduplicated.columns:\n",
        "        agg_expressions.append(countDistinct(\"hs9\").alias(\"unique_hs9\"))\n",
        "    \n",
        "    if \"country\" in df_deduplicated.columns:\n",
        "        agg_expressions.append(countDistinct(\"country\").alias(\"unique_countries\"))\n",
        "    \n",
        "    numeric_cols = [\"customs\", \"q1\", \"q2\", \"value_yen\"]\n",
        "    for col_name in numeric_cols:\n",
        "        if col_name in df_deduplicated.columns:\n",
        "            agg_expressions.extend([\n",
        "                avg(col_name).alias(f\"avg_{col_name}\"),\n",
        "                expr(f\"percentile_approx({col_name}, 0.5)\").alias(f\"median_{col_name}\")\n",
        "            ])\n",
        "    \n",
        "    agg_expressions.append(count(\"*\").alias(\"total_records\"))\n",
        "    \n",
        "    time_aggregation = df_time_partitioned.groupBy(\"year\", \"month\").agg(*agg_expressions).orderBy(\"year\", \"month\").cache()\n",
        "    \n",
        "    print(\"Агрегация по времени:\")\n",
        "    time_aggregation.show(50)\n",
        "    \n",
        "    print(f\"Результат агрегации: {time_aggregation.count():,} записей\")\n",
        "    print(f\"Партиций в результате: {time_aggregation.rdd.getNumPartitions()}\")\n",
        "    \n",
        "else:\n",
        "    print(\"Временные колонки не найдены\")\n",
        "    time_aggregation = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "\n",
        "numeric_column = \"value_yen\"\n",
        "\n",
        "if numeric_column in df_with_metrics.columns:\n",
        "    \n",
        "    numeric_data = df_with_metrics.select(numeric_column).filter(col(numeric_column).isNotNull()).cache()\n",
        "    \n",
        "    print(f\"\\nАнализ колонки: {numeric_column}\")\n",
        "    print(f\"Количество записей: {numeric_data.count():,}\")\n",
        "    \n",
        "    # 1. Гистограмма - подготовка данных\n",
        "    \n",
        "    histogram_sample = numeric_data.sample(0.01, seed=42).cache()\n",
        "    sample_count = histogram_sample.count()\n",
        "    print(f\"\\nРазмер выборки для гистограммы: {sample_count:,}\")\n",
        "    \n",
        "    hist_stats = histogram_sample.agg(\n",
        "        min(numeric_column).alias(\"min_val\"),\n",
        "        max(numeric_column).alias(\"max_val\"),\n",
        "        avg(numeric_column).alias(\"mean_val\"),\n",
        "        stddev(numeric_column).alias(\"std_val\")\n",
        "    ).collect()[0]\n",
        "    \n",
        "    print(f\"Диапазон: [{hist_stats['min_val']:,}, {hist_stats['max_val']:,}]\")\n",
        "    print(f\"Среднее: {hist_stats['mean_val']:,.2f}\")\n",
        "    print(f\"Стандартное отклонение: {hist_stats['std_val']:,.2f}\")\n",
        "    \n",
        "    hist_data = histogram_sample.rdd.map(lambda row: float(row[0])).histogram(50)\n",
        "    bins = hist_data[0]\n",
        "    frequencies = hist_data[1]\n",
        "    \n",
        "    print(f\"Гистограмма создана: {len(bins)-1} интервалов\")\n",
        "    print(\"Первые 10 интервалов и частот:\")\n",
        "    \n",
        "    num_to_show = 10\n",
        "    if len(frequencies) < 10:\n",
        "        num_to_show = len(frequencies)\n",
        "    \n",
        "    for i in range(num_to_show):\n",
        "        print(f\"   [{bins[i]:,.0f}, {bins[i+1]:,.0f}): {frequencies[i]:,}\")\n",
        "    \n",
        "    # 2. 95% доверительный интервал\n",
        "    \n",
        "    print(f\"\\n2. 95% доверительный интервал для {numeric_column}:\")\n",
        "    \n",
        "    full_stats = numeric_data.agg(\n",
        "        count(numeric_column).alias(\"n\"),\n",
        "        avg(numeric_column).alias(\"mean\"),\n",
        "        stddev(numeric_column).alias(\"stddev\")\n",
        "    ).collect()[0]\n",
        "    \n",
        "    n = full_stats[\"n\"]\n",
        "    mean_val = full_stats[\"mean\"]\n",
        "    std_val = full_stats[\"stddev\"]\n",
        "    \n",
        "    print(f\"Полная выборка: n={n:,}, mean={mean_val:.2f}, std={std_val:.2f}\")\n",
        "    \n",
        "    \n",
        "    print(f\"\\nМетодика расчета доверительного интервала:\")\n",
        "    print(f\"Размер выборки: n={n:,}\")\n",
        "    \n",
        "    if n >= 30:\n",
        "        print(\"n >= 30\")\n",
        "        print(\"Используем нормальное распределение с z-критическим значением\")\n",
        "        print(\"Для 95%: z = 1.96\")\n",
        "        \n",
        "        z_critical = 1.96\n",
        "        margin_error = z_critical * (std_val / math.sqrt(n))\n",
        "        ci_lower = mean_val - margin_error\n",
        "        ci_upper = mean_val + margin_error\n",
        "        \n",
        "        method = \"ЦПТ + нормальное распределение\"\n",
        "        \n",
        "    else:\n",
        "        print(\"n < 30 \") \n",
        "        print(\"df = n-1\")\n",
        "        print(\"4. Для 95%: t-критическое ≈ 2.0\")\n",
        "        \n",
        "        t_critical = 2.0\n",
        "        margin_error = t_critical * (std_val / math.sqrt(n))\n",
        "        ci_lower = mean_val - margin_error\n",
        "        ci_upper = mean_val + margin_error\n",
        "        \n",
        "        method = \"t-распределение\"\n",
        "    \n",
        "    print(f\"\\nРезультат 95% доверительного интервала:\")\n",
        "    print(f\"   Метод: {method}\")\n",
        "    print(f\"   Интервал: [{ci_lower:,.2f}, {ci_upper:,.2f}]\")\n",
        "    print(f\"   Ширина интервала: {ci_upper - ci_lower:,.2f}\")\n",
        "    print(f\"   Погрешность: ±{margin_error:,.2f}\")\n",
        "    \n",
        "    print(f\"\\nС вероятностью 95% истинное среднее значение {numeric_column}\")\n",
        "    print(f\"находится в интервале [{ci_lower:,.2f}, {ci_upper:,.2f}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "\n",
        "if time_aggregation is not None and f\"avg_{numeric_column}\" in time_aggregation.columns:\n",
        "    \n",
        "    # Подготавливаем данные для графика\n",
        "    monthly_avg_data = time_aggregation.select(\"year\", \"month\", f\"avg_{numeric_column}\", \"total_records\").orderBy(\"year\", \"month\")\n",
        "    \n",
        "    \n",
        "    chart_data = monthly_avg_data.withColumn(\"period\", \n",
        "                                            concat(col(\"year\").cast(\"string\"), \n",
        "                                                  lit(\"-\"), \n",
        "                                                  lpad(col(\"month\"), 2, \"0\")))\n",
        "    \n",
        "    # Собираем данные\n",
        "    rows = chart_data.collect()\n",
        "    \n",
        "    print(\"%table period\\taverage_value\\ttotal_records\")\n",
        "    for row in rows:\n",
        "        period = row.period\n",
        "        avg_val = getattr(row, f\"avg_{numeric_column}\")\n",
        "        total = row.total_records\n",
        "        print(f\"{period}\\t{avg_val:.2f}\\t{total}\")\n",
        "    \n",
        "    print(f\"\\nПодготовлено {len(rows)} точек для графика\")\n",
        "\n",
        "else:\n",
        "    print(\"Данные для графика среднего значения не найдены\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "if time_aggregation is not None:\n",
        "    df_for_join = df_deduplicated.repartition(\"year\", \"month\")\n",
        "    time_agg_for_join = time_aggregation.repartition(\"year\", \"month\")\n",
        "    \n",
        "    print(\"Данные перепартиционированы для оптимального join\")\n",
        "    \n",
        "    df_with_metrics = df_for_join.join(time_agg_for_join, [\"year\", \"month\"], \"left\").cache()\n",
        "    \n",
        "    print(f\"Датасет с метриками: {df_with_metrics.count():,} записей\")\n",
        "    print(f\"Партиций после join: {df_with_metrics.rdd.getNumPartitions()}\")\n",
        "    \n",
        "    print(\"Образец данных с метриками:\")\n",
        "    sample_cols = [col for col in [\"year\", \"month\", \"value_yen\", \"unique_countries\", \"avg_value_yen\"] if col in df_with_metrics.columns]\n",
        "    df_with_metrics.select(*sample_cols).show(10)\n",
        "    \n",
        "    null_metrics_count = df_with_metrics.filter(col(\"total_records\").isNull()).count()\n",
        "    print(f\"Записей без метрик: {null_metrics_count:,}\")\n",
        "    \n",
        "else:\n",
        "    df_with_metrics = df_deduplicated\n",
        "    print(\"Используем данные без временных метрик\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "\n",
        "df_sample1, df_sample2, df_sample3 = df_with_metrics.randomSplit([0.25, 0.25, 0.5], seed=42)\n",
        "\n",
        "df_sample1.cache()\n",
        "df_sample2.cache() \n",
        "df_sample3.cache()\n",
        "\n",
        "count1 = df_sample1.count()\n",
        "count2 = df_sample2.count()\n",
        "count3 = df_sample3.count()\n",
        "total = count1 + count2 + count3\n",
        "\n",
        "print(f\"РАЗДЕЛЕНИЕ ДАТАСЕТА:\")\n",
        "print(f\"   Выборка 1: {count1:,} записей ({count1/total*100:.1f}%)\")\n",
        "print(f\"   Выборка 2: {count2:,} записей ({count2/total*100:.1f}%)\")\n",
        "print(f\"   Выборка 3: {count3:,} записей ({count3/total*100:.1f}%)\")\n",
        "print(f\"   Всего: {total:,} записей\")\n",
        "\n",
        "print(f\"Партиции в выборках:\")\n",
        "print(f\"   Выборка 1: {df_sample1.rdd.getNumPartitions()} партиций\")\n",
        "print(f\"   Выборка 2: {df_sample2.rdd.getNumPartitions()} партиций\")\n",
        "print(f\"   Выборка 3: {df_sample3.rdd.getNumPartitions()} партиций\")\n",
        "\n",
        "print(\"\\nРаспределение по годам в выборках:\")\n",
        "for i, df_sample in enumerate([df_sample1, df_sample2, df_sample3], 1):\n",
        "    if \"year\" in df_sample.columns:\n",
        "        year_dist = df_sample.groupBy(\"year\").count().orderBy(\"year\")\n",
        "        print(f\"\\nВыборка {i} - топ-10 годов:\")\n",
        "        year_dist.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": [
        "%pyspark\n",
        "\n",
        "# Данные конкурента\n",
        "competitor_successes = 5\n",
        "competitor_failures = 995\n",
        "\n",
        "# Данные компании\n",
        "our_failures = 200\n",
        "\n",
        "# Априорное распределение на основе данных конкурента\n",
        "alpha_prior = competitor_successes\n",
        "beta_prior = competitor_failures\n",
        "\n",
        "# Апостериорное распределение после наших наблюдений\n",
        "alpha_posterior = alpha_prior + 0 \n",
        "beta_posterior = beta_prior + our_failures\n",
        "\n",
        "# Вероятность успеха следующего прототипа\n",
        "probability_success = alpha_posterior / (alpha_posterior + beta_posterior)\n",
        "\n",
        "print(f\"Вероятность успеха 201-го прототипа: {probability_success:.4f} или {probability_success*100:.2f}%\")\n",
        "\n",
        "# Доверительный интервал\n",
        "beta_dist = stats.beta(alpha_posterior, beta_posterior)\n",
        "ci_lower, ci_upper = beta_dist.interval(0.95)\n",
        "print(f\"95% доверительный интервал: [{ci_lower:.4f}, {ci_upper:.4f}]\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala",
      "display_name": "Apache Toree - Scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    },
    "name": "parallel_code_dataset"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}