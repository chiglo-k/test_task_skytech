{
  "metadata": {
    "name": "parallel_code_dataset",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala",
      "display_name": "Apache Toree - Scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import concat, lpad, lit\nimport pandas as pd\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Проверка кластера\nprint(f\"Spark версия: {spark.version}\")\nprint(f\"Доступно ядер: {spark.sparkContext.defaultParallelism}\")\nprint(f\"Мастер URL: {spark.sparkContext.master}\")\n\n# Информация о кластере\nprint(f\"Executor instances: {spark.conf.get('spark.executor.instances', 'auto')}\")\nprint(f\"Executor memory: {spark.conf.get('spark.executor.memory', 'default')}\")"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nspark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\nspark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\nspark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\nspark.conf.set(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\")\nspark.conf.set(\"spark.sql.shuffle.partitions\", \"400\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nbucket_name = 'sparkwrk'\n\ndf = spark.read.option(\"header\", True).csv(f's3a://{bucket_name}/bi_datasets/custom_1988_2020.csv')  # Датасет по трейдингу (Спасибо, Японскому правительству!)"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\ninitial_partitions = df.rdd.getNumPartitions()\n\nprint(f\"Исходное количество партиций: {initial_partitions}\")\n\nprint(\"Оригинальные колонки:\")\nall_columns = df.columns\nfor i, col_name in enumerate(all_columns):\n    print(f\"   {i}: '{col_name}'\")\n\nprint(f\"\\nВсего колонок: {len(all_columns)}\")\n\nrename_dict = {}\nfor col_name in all_columns:\n    if \"198801\" in str(col_name) or col_name == \"198801\":\n        rename_dict[col_name] = \"ym\"\n    elif col_name == \"1\":\n        rename_dict[col_name] = \"exp_imp\"  \n    elif col_name == \"103\":\n        rename_dict[col_name] = \"hs9\"\n    elif col_name == \"100\":\n        rename_dict[col_name] = \"customs\"\n    elif \"000000190\" in str(col_name) or col_name == \"000000190\":\n        rename_dict[col_name] = \"country\"\n    elif col_name == \"0\":\n        rename_dict[col_name] = \"q1\"\n    elif col_name == \"35843\":\n        rename_dict[col_name] = \"q2\"  \n    elif col_name == \"34353\":\n        rename_dict[col_name] = \"value_yen\"\n\nprint(f\"\\nСоответствия:\")\n\nfor old_name, new_name in rename_dict.items():\n    print(f\"   '{old_name}' -> '{new_name}'\")\n\ndf_processed = df.select([col(c).alias(rename_dict.get(c, c)) for c in df.columns])\n\n\nif initial_partitions * 2 > 200:\n    optimal_partitions = initial_partitions * 2\nelse:\n    optimal_partitions = 200\n\ndf_processed = df_processed.repartition(optimal_partitions)\nprint(f\"Оптимизированное количество партиций: {df_processed.rdd.getNumPartitions()}\")\n\n# Приводим типы данных\nif \"ym\" in df_processed.columns:\n    df_processed = df_processed.withColumn(\"ym\", col(\"ym\").cast(IntegerType()))\n    df_processed = df_processed.withColumn(\"year\", (col(\"ym\") / 100).cast(IntegerType()))\n    df_processed = df_processed.withColumn(\"month\", when((col(\"ym\") % 100) < 10, concat(lit(\"0\"), (col(\"ym\") % 100).cast(StringType()))).otherwise((col(\"ym\") % 100).cast(StringType())))\n\nif \"exp_imp\" in df_processed.columns:\n    df_processed = df_processed.withColumn(\"exp_imp\", col(\"exp_imp\").cast(StringType()))\n\nif \"hs9\" in df_processed.columns:\n    df_processed = df_processed.withColumn(\"hs9\", col(\"hs9\").cast(StringType()))\n\nif \"customs\" in df_processed.columns:\n    df_processed = df_processed.withColumn(\"customs\", col(\"customs\").cast(IntegerType()))\n\nif \"country\" in df_processed.columns:\n    df_processed = df_processed.withColumn(\"country\", col(\"country\").cast(StringType()))\n\nif \"q1\" in df_processed.columns:\n    df_processed = df_processed.withColumn(\"q1\", col(\"q1\").cast(LongType()))\n\nif \"q2\" in df_processed.columns:\n    df_processed = df_processed.withColumn(\"q2\", col(\"q2\").cast(LongType()))\n\nif \"value_yen\" in df_processed.columns:\n    df_processed = df_processed.withColumn(\"value_yen\", col(\"value_yen\").cast(LongType()))\n\ndf_processed.cache()\n\nprint(\"Схема после обработки:\")\ndf_processed.printSchema()\n\nprint(\"Образец данных:\")\navailable_cols = [col for col in [\"year\", \"month\", \"exp_imp\", \"hs9\", \"country\", \"q1\", \"q2\", \"value_yen\"] if col in df_processed.columns]\ndf_processed.select(*available_cols).show(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n\nfilters = []\n\nif \"year\" in df_processed.columns and \"month\" in df_processed.columns:\n    filters.append(col(\"year\").isNotNull() & col(\"month\").isNotNull())\n\nif \"exp_imp\" in df_processed.columns:\n    filters.append(col(\"exp_imp\").isNotNull() & (col(\"exp_imp\") != \"\"))\n\nif \"hs9\" in df_processed.columns:\n    filters.append(col(\"hs9\").isNotNull() & (col(\"hs9\") != \"\"))\n\nif \"country\" in df_processed.columns:\n    filters.append(col(\"country\").isNotNull() & (col(\"country\") != \"\"))\n\nif \"value_yen\" in df_processed.columns:\n    filters.append(col(\"value_yen\").isNotNull() & (col(\"value_yen\") > 0))\n\nif \"q1\" in df_processed.columns:\n    filters.append(col(\"q1\").isNotNull())\n\nif \"q2\" in df_processed.columns:\n    filters.append(col(\"q2\").isNotNull())\n\n# Объединяем все фильтры\ncombined_filter = filters[0]\nfor filter_condition in filters[1:]:\n    combined_filter = combined_filter & filter_condition\n\ndf_clean = df_processed.filter(combined_filter)\nprint(f\"После удаления пустых: {df_clean.count():,} записей\")\n\n# Фильтруем строки с цифрами в hs9\nif \"hs9\" in df_clean.columns:\n    df_with_digits = df_clean.filter(col(\"hs9\").rlike(\".*[0-9].*\"))\n    print(f\"После фильтрации строк с цифрами: {df_with_digits.count():,} записей\")\nelse:\n    df_with_digits = df_clean\n    print(\"Колонка hs9 не найдена\")\n\n# Поиск дублей\nkey_fields = [field for field in [\"year\", \"month\", \"exp_imp\", \"hs9\", \"country\", \"customs\"] if field in df_with_digits.columns]\nprint(f\"Ключевые поля для дедупликации: {key_fields}\")\n\nif len(key_fields) > 0:\n    if \"country\" in key_fields:\n        df_partitioned = df_with_digits.repartition(\"country\")\n        print(\"Данные перепартиционированы по country\")\n    else:\n        df_partitioned = df_with_digits\n    \n    print(\"Анализ дублей с параллелизмом...\")\n    duplicates = df_partitioned.groupBy(*key_fields).count().filter(col(\"count\") > 1).cache()\n    \n    duplicate_count = duplicates.count()\n    total_duplicate_records = duplicates.agg(sum(\"count\")).collect()[0][0] if duplicate_count > 0 else 0\n    \n    print(f\"Найдено групп дублей: {duplicate_count:,}\")\n    print(f\"Всего дублированных записей: {total_duplicate_records:,}\")\n    \n    if duplicate_count > 0:\n        print(\"Топ-10 дублей:\")\n        duplicates.orderBy(desc(\"count\")).show(10)\n    \n    df_deduplicated = df_partitioned.dropDuplicates(key_fields)\nelse:\n    df_deduplicated = df_with_digits\n\nfinal_count = df_deduplicated.count()\nprint(f\"Финальный датасет: {final_count:,} записей\")\nprint(f\"Финальное количество партиций: {df_deduplicated.rdd.getNumPartitions()}\")\n\ndf_deduplicated.cache()"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n\nif \"year\" in df_deduplicated.columns and \"month\" in df_deduplicated.columns:\n    \n    df_time_partitioned = df_deduplicated.repartition(\"year\", \"month\")\n    print(f\"Данные перепартиционированы по времени: {df_time_partitioned.rdd.getNumPartitions()} партиций\")\n    \n    agg_expressions = []\n    \n    if \"exp_imp\" in df_deduplicated.columns:\n        agg_expressions.append(countDistinct(\"exp_imp\").alias(\"unique_exp_imp\"))\n    \n    if \"hs9\" in df_deduplicated.columns:\n        agg_expressions.append(countDistinct(\"hs9\").alias(\"unique_hs9\"))\n    \n    if \"country\" in df_deduplicated.columns:\n        agg_expressions.append(countDistinct(\"country\").alias(\"unique_countries\"))\n    \n    numeric_cols = [\"customs\", \"q1\", \"q2\", \"value_yen\"]\n    for col_name in numeric_cols:\n        if col_name in df_deduplicated.columns:\n            agg_expressions.extend([\n                avg(col_name).alias(f\"avg_{col_name}\"),\n                expr(f\"percentile_approx({col_name}, 0.5)\").alias(f\"median_{col_name}\")\n            ])\n    \n    agg_expressions.append(count(\"*\").alias(\"total_records\"))\n    \n    time_aggregation = df_time_partitioned.groupBy(\"year\", \"month\").agg(*agg_expressions).orderBy(\"year\", \"month\").cache()\n    \n    print(\"Агрегация по времени:\")\n    time_aggregation.show(50)\n    \n    print(f\"Результат агрегации: {time_aggregation.count():,} записей\")\n    print(f\"Партиций в результате: {time_aggregation.rdd.getNumPartitions()}\")\n    \nelse:\n    print(\"Временные колонки не найдены\")\n    time_aggregation = None"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n\nnumeric_column = \"value_yen\"\n\nif numeric_column in df_with_metrics.columns:\n    \n    numeric_data = df_with_metrics.select(numeric_column).filter(col(numeric_column).isNotNull()).cache()\n    \n    print(f\"\\nАнализ колонки: {numeric_column}\")\n    print(f\"Количество записей: {numeric_data.count():,}\")\n    \n    # 1. Гистограмма - подготовка данных\n    \n    histogram_sample = numeric_data.sample(0.01, seed=42).cache()\n    sample_count = histogram_sample.count()\n    print(f\"\\nРазмер выборки для гистограммы: {sample_count:,}\")\n    \n    hist_stats = histogram_sample.agg(\n        min(numeric_column).alias(\"min_val\"),\n        max(numeric_column).alias(\"max_val\"),\n        avg(numeric_column).alias(\"mean_val\"),\n        stddev(numeric_column).alias(\"std_val\")\n    ).collect()[0]\n    \n    print(f\"Диапазон: [{hist_stats['min_val']:,}, {hist_stats['max_val']:,}]\")\n    print(f\"Среднее: {hist_stats['mean_val']:,.2f}\")\n    print(f\"Стандартное отклонение: {hist_stats['std_val']:,.2f}\")\n    \n    hist_data = histogram_sample.rdd.map(lambda row: float(row[0])).histogram(50)\n    bins = hist_data[0]\n    frequencies = hist_data[1]\n    \n    print(f\"Гистограмма создана: {len(bins)-1} интервалов\")\n    print(\"Первые 10 интервалов и частот:\")\n    \n    num_to_show = 10\n    if len(frequencies) < 10:\n        num_to_show = len(frequencies)\n    \n    for i in range(num_to_show):\n        print(f\"   [{bins[i]:,.0f}, {bins[i+1]:,.0f}): {frequencies[i]:,}\")\n    \n    # 2. 95% доверительный интервал\n    \n    print(f\"\\n2. 95% доверительный интервал для {numeric_column}:\")\n    \n    full_stats = numeric_data.agg(\n        count(numeric_column).alias(\"n\"),\n        avg(numeric_column).alias(\"mean\"),\n        stddev(numeric_column).alias(\"stddev\")\n    ).collect()[0]\n    \n    n = full_stats[\"n\"]\n    mean_val = full_stats[\"mean\"]\n    std_val = full_stats[\"stddev\"]\n    \n    print(f\"Полная выборка: n={n:,}, mean={mean_val:.2f}, std={std_val:.2f}\")\n    \n    \n    print(f\"\\nМетодика расчета доверительного интервала:\")\n    print(f\"Размер выборки: n={n:,}\")\n    \n    if n >= 30:\n        print(\"n >= 30\")\n        print(\"Используем нормальное распределение с z-критическим значением\")\n        print(\"Для 95%: z = 1.96\")\n        \n        z_critical = 1.96\n        margin_error = z_critical * (std_val / math.sqrt(n))\n        ci_lower = mean_val - margin_error\n        ci_upper = mean_val + margin_error\n        \n        method = \"ЦПТ + нормальное распределение\"\n        \n    else:\n        print(\"n < 30 \") \n        print(\"df = n-1\")\n        print(\"4. Для 95%: t-критическое ≈ 2.0\")\n        \n        t_critical = 2.0\n        margin_error = t_critical * (std_val / math.sqrt(n))\n        ci_lower = mean_val - margin_error\n        ci_upper = mean_val + margin_error\n        \n        method = \"t-распределение\"\n    \n    print(f\"\\nРезультат 95% доверительного интервала:\")\n    print(f\"   Метод: {method}\")\n    print(f\"   Интервал: [{ci_lower:,.2f}, {ci_upper:,.2f}]\")\n    print(f\"   Ширина интервала: {ci_upper - ci_lower:,.2f}\")\n    print(f\"   Погрешность: ±{margin_error:,.2f}\")\n    \n    print(f\"\\nС вероятностью 95% истинное среднее значение {numeric_column}\")\n    print(f\"находится в интервале [{ci_lower:,.2f}, {ci_upper:,.2f}]\")"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n\nif time_aggregation is not None and f\"avg_{numeric_column}\" in time_aggregation.columns:\n    \n    # Подготавливаем данные для графика\n    monthly_avg_data = time_aggregation.select(\"year\", \"month\", f\"avg_{numeric_column}\", \"total_records\").orderBy(\"year\", \"month\")\n    \n    \n    chart_data = monthly_avg_data.withColumn(\"period\", \n                                            concat(col(\"year\").cast(\"string\"), \n                                                  lit(\"-\"), \n                                                  lpad(col(\"month\"), 2, \"0\")))\n    \n    # Собираем данные\n    rows = chart_data.collect()\n    \n    print(\"%table period\\taverage_value\\ttotal_records\")\n    for row in rows:\n        period = row.period\n        avg_val = getattr(row, f\"avg_{numeric_column}\")\n        total = row.total_records\n        print(f\"{period}\\t{avg_val:.2f}\\t{total}\")\n    \n    print(f\"\\nПодготовлено {len(rows)} точек для графика\")\n\nelse:\n    print(\"Данные для графика среднего значения не найдены\")"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nif time_aggregation is not None:\n    df_for_join = df_deduplicated.repartition(\"year\", \"month\")\n    time_agg_for_join = time_aggregation.repartition(\"year\", \"month\")\n    \n    print(\"Данные перепартиционированы для оптимального join\")\n    \n    df_with_metrics = df_for_join.join(time_agg_for_join, [\"year\", \"month\"], \"left\").cache()\n    \n    print(f\"Датасет с метриками: {df_with_metrics.count():,} записей\")\n    print(f\"Партиций после join: {df_with_metrics.rdd.getNumPartitions()}\")\n    \n    print(\"Образец данных с метриками:\")\n    sample_cols = [col for col in [\"year\", \"month\", \"value_yen\", \"unique_countries\", \"avg_value_yen\"] if col in df_with_metrics.columns]\n    df_with_metrics.select(*sample_cols).show(10)\n    \n    null_metrics_count = df_with_metrics.filter(col(\"total_records\").isNull()).count()\n    print(f\"Записей без метрик: {null_metrics_count:,}\")\n    \nelse:\n    df_with_metrics = df_deduplicated\n    print(\"Используем данные без временных метрик\")"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n\ndf_sample1, df_sample2, df_sample3 = df_with_metrics.randomSplit([0.25, 0.25, 0.5], seed=42)\n\ndf_sample1.cache()\ndf_sample2.cache() \ndf_sample3.cache()\n\ncount1 = df_sample1.count()\ncount2 = df_sample2.count()\ncount3 = df_sample3.count()\ntotal = count1 + count2 + count3\n\nprint(f\"РАЗДЕЛЕНИЕ ДАТАСЕТА:\")\nprint(f\"   Выборка 1: {count1:,} записей ({count1/total*100:.1f}%)\")\nprint(f\"   Выборка 2: {count2:,} записей ({count2/total*100:.1f}%)\")\nprint(f\"   Выборка 3: {count3:,} записей ({count3/total*100:.1f}%)\")\nprint(f\"   Всего: {total:,} записей\")\n\nprint(f\"Партиции в выборках:\")\nprint(f\"   Выборка 1: {df_sample1.rdd.getNumPartitions()} партиций\")\nprint(f\"   Выборка 2: {df_sample2.rdd.getNumPartitions()} партиций\")\nprint(f\"   Выборка 3: {df_sample3.rdd.getNumPartitions()} партиций\")\n\nprint(\"\\nРаспределение по годам в выборках:\")\nfor i, df_sample in enumerate([df_sample1, df_sample2, df_sample3], 1):\n    if \"year\" in df_sample.columns:\n        year_dist = df_sample.groupBy(\"year\").count().orderBy(\"year\")\n        print(f\"\\nВыборка {i} - топ-10 годов:\")\n        year_dist.show(10)"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Данные конкурента\ncompetitor_successes = 5\ncompetitor_failures = 995\n\n# Данные компании\nour_failures = 200\n\n# Априорное распределение на основе данных конкурента\nalpha_prior = competitor_successes\nbeta_prior = competitor_failures\n\n# Апостериорное распределение после наших наблюдений\nalpha_posterior = alpha_prior + 0 \nbeta_posterior = beta_prior + our_failures\n\n# Вероятность успеха следующего прототипа\nprobability_success = alpha_posterior / (alpha_posterior + beta_posterior)\n\nprint(f\"Вероятность успеха 201-го прототипа: {probability_success:.4f} или {probability_success*100:.2f}%\")\n\n# Доверительный интервал\nbeta_dist = stats.beta(alpha_posterior, beta_posterior)\nci_lower, ci_upper = beta_dist.interval(0.95)\nprint(f\"95% доверительный интервал: [{ci_lower:.4f}, {ci_upper:.4f}]\")"
    }
  ]
}