#–¢–µ—Å—Ç–æ–≤–æ–µ –∑–∞–¥–∞–Ω–∏–µ

## –û–±—â–µ–µ –≤—Ä–µ–º—è: 6 —á–∞—Å–æ–≤ 30 –º–∏–Ω—É—Ç

    üöÄ 30 –º–∏–Ω—É—Ç - –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±–ª–∞—á–Ω–æ–π –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã
    üìä 6 —á–∞—Å–æ–≤ - –∞–Ω–∞–ª–∏–∑ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö

## –ò–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞
–û–±–ª–∞—á–Ω–∞—è –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞

    –ü—Ä–æ–≤–∞–π–¥–µ—Ä: Yandex Cloud
    –°–µ—Ä–≤–∏—Å: DataProc (—É–ø—Ä–∞–≤–ª—è–µ–º—ã–π Apache Spark)
    –¢–µ—Ö–Ω–æ–ª–æ–≥–∏—è: PySpark 3.0.3

–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–∞

    Executor instances: 2
    Executor memory: 5530m
    –î–æ—Å—Ç—É–ø–Ω–æ —è–¥–µ—Ä: 2
    –ú–∞—Å—Ç–µ—Ä URL: yarn

–î–∞—Ç–∞—Å–µ—Ç

–ò—Å—Ç–æ—á–Ω–∏–∫: 100 Million Data CSV (https://www.kaggle.com/datasets/zanjibar/100-million-data-csv/data)

–û–ø–∏—Å–∞–Ω–∏–µ: –¢–æ—Ä–≥–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –Ø–ø–æ–Ω–∏–∏ –∑–∞ –ø–µ—Ä–∏–æ–¥ 1988-2020 –≥–æ–¥–æ–≤

    –§–∞–π–ª: custom_1988_2020.csv
    –†–∞–∑–º–µ—Ä: 113,607,321 –∑–∞–ø–∏—Å–µ–π
    –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ: –î–∞–Ω–Ω—ã–µ –ø–æ —ç–∫—Å–ø–æ—Ä—Ç—É/–∏–º–ø–æ—Ä—Ç—É, —Ç–æ–≤–∞—Ä–Ω—ã–µ –∫–æ–¥—ã, —Å—Ç—Ä–∞–Ω—ã, —Å—Ç–æ–∏–º–æ—Å—Ç—å –≤ –π–µ–Ω–∞—Ö

## –ö–æ–¥
### –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞

```python
%pyspark
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql.functions import concat, lpad, lit
import pandas as pd
import math
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Spark –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
spark.conf.set("spark.sql.adaptive.skewJoin.enabled", "true")
spark.conf.set("spark.sql.adaptive.localShuffleReader.enabled", "true")
spark.conf.set("spark.sql.shuffle.partitions", "400")

bucket_name = 'sparkwrk'

df = spark.read.option("header", True).csv(f's3a://{bucket_name}/bi_datasets/custom_1988_2020.csv')
df.printSchema()
```
![](/screen/1.png)

### –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö

```python
%pyspark

initial_partitions = df.rdd.getNumPartitions()

print(f"–ò—Å—Ö–æ–¥–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä—Ç–∏—Ü–∏–π: {initial_partitions}")

print("–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏:")
all_columns = df.columns
for i, col_name in enumerate(all_columns):
    print(f"   {i}: '{col_name}'")

print(f"\n–í—Å–µ–≥–æ –∫–æ–ª–æ–Ω–æ–∫: {len(all_columns)}")

rename_dict = {}
for col_name in all_columns:
    if "198801" in str(col_name) or col_name == "198801":
        rename_dict[col_name] = "ym"
    elif col_name == "1":
        rename_dict[col_name] = "exp_imp"  
    elif col_name == "103":
        rename_dict[col_name] = "hs9"
    elif col_name == "100":
        rename_dict[col_name] = "customs"
    elif "000000190" in str(col_name) or col_name == "000000190":
        rename_dict[col_name] = "country"
    elif col_name == "0":
        rename_dict[col_name] = "q1"
    elif col_name == "35843":
        rename_dict[col_name] = "q2"  
    elif col_name == "34353":
        rename_dict[col_name] = "value_yen"

print(f"\n–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è:")

for old_name, new_name in rename_dict.items():
    print(f"   '{old_name}' -> '{new_name}'")

df_processed = df.select([col(c).alias(rename_dict.get(c, c)) for c in df.columns])


if initial_partitions * 2 > 200:
    optimal_partitions = initial_partitions * 2
else:
    optimal_partitions = 200

df_processed = df_processed.repartition(optimal_partitions)
print(f"–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä—Ç–∏—Ü–∏–π: {df_processed.rdd.getNumPartitions()}")

# –ü—Ä–∏–≤–æ–¥–∏–º —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö
if "ym" in df_processed.columns:
    df_processed = df_processed.withColumn("ym", col("ym").cast(IntegerType()))
    df_processed = df_processed.withColumn("year", (col("ym") / 100).cast(IntegerType()))
    df_processed = df_processed.withColumn("month", when((col("ym") % 100) < 10, concat(lit("0"), (col("ym") % 100).cast(StringType()))).otherwise((col("ym") % 100).cast(StringType())))

if "exp_imp" in df_processed.columns:
    df_processed = df_processed.withColumn("exp_imp", col("exp_imp").cast(StringType()))

if "hs9" in df_processed.columns:
    df_processed = df_processed.withColumn("hs9", col("hs9").cast(StringType()))

if "customs" in df_processed.columns:
    df_processed = df_processed.withColumn("customs", col("customs").cast(IntegerType()))

if "country" in df_processed.columns:
    df_processed = df_processed.withColumn("country", col("country").cast(StringType()))

if "q1" in df_processed.columns:
    df_processed = df_processed.withColumn("q1", col("q1").cast(LongType()))

if "q2" in df_processed.columns:
    df_processed = df_processed.withColumn("q2", col("q2").cast(LongType()))

if "value_yen" in df_processed.columns:
    df_processed = df_processed.withColumn("value_yen", col("value_yen").cast(LongType()))

df_processed.cache()

print("–°—Ö–µ–º–∞ –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏:")
df_processed.printSchema()

print("–û–±—Ä–∞–∑–µ—Ü –¥–∞–Ω–Ω—ã—Ö:")
available_cols = [col for col in ["year", "month", "exp_imp", "hs9", "country", "q1", "q2", "value_yen"] if col in df_processed.columns]
df_processed.select(*available_cols).show(5)
```
![](/screen/2.png)

![](/screen/3.png)

![](/screen/4.png)

![](/screen/5.png)

### –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö

```python
%pyspark


filters = []

if "year" in df_processed.columns and "month" in df_processed.columns:
    filters.append(col("year").isNotNull() & col("month").isNotNull())

if "exp_imp" in df_processed.columns:
    filters.append(col("exp_imp").isNotNull() & (col("exp_imp") != ""))

if "hs9" in df_processed.columns:
    filters.append(col("hs9").isNotNull() & (col("hs9") != ""))

if "country" in df_processed.columns:
    filters.append(col("country").isNotNull() & (col("country") != ""))

if "value_yen" in df_processed.columns:
    filters.append(col("value_yen").isNotNull() & (col("value_yen") > 0))

if "q1" in df_processed.columns:
    filters.append(col("q1").isNotNull())

if "q2" in df_processed.columns:
    filters.append(col("q2").isNotNull())

# –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Ñ–∏–ª—å—Ç—Ä—ã
combined_filter = filters[0]
for filter_condition in filters[1:]:
    combined_filter = combined_filter & filter_condition

df_clean = df_processed.filter(combined_filter)
print(f"–ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –ø—É—Å—Ç—ã—Ö: {df_clean.count():,} –∑–∞–ø–∏—Å–µ–π")

# –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Ç—Ä–æ–∫–∏ —Å —Ü–∏—Ñ—Ä–∞–º–∏ –≤ hs9
if "hs9" in df_clean.columns:
    df_with_digits = df_clean.filter(col("hs9").rlike(".*[0-9].*"))
    print(f"–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Å—Ç—Ä–æ–∫ —Å —Ü–∏—Ñ—Ä–∞–º–∏: {df_with_digits.count():,} –∑–∞–ø–∏—Å–µ–π")
else:
    df_with_digits = df_clean
    print("–ö–æ–ª–æ–Ω–∫–∞ hs9 –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")

# –ü–æ–∏—Å–∫ –¥—É–±–ª–µ–π
key_fields = [field for field in ["year", "month", "exp_imp", "hs9", "country", "customs"] if field in df_with_digits.columns]
print(f"–ö–ª—é—á–µ–≤—ã–µ –ø–æ–ª—è –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏: {key_fields}")

if len(key_fields) > 0:
    if "country" in key_fields:
        df_partitioned = df_with_digits.repartition("country")
        print("–î–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ–ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω—ã –ø–æ country")
    else:
        df_partitioned = df_with_digits
    
    print("–ê–Ω–∞–ª–∏–∑ –¥—É–±–ª–µ–π —Å –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–æ–º...")
    duplicates = df_partitioned.groupBy(*key_fields).count().filter(col("count") > 1).cache()
    
    duplicate_count = duplicates.count()
    total_duplicate_records = duplicates.agg(sum("count")).collect()[0][0] if duplicate_count > 0 else 0
    
    print(f"–ù–∞–π–¥–µ–Ω–æ –≥—Ä—É–ø–ø –¥—É–±–ª–µ–π: {duplicate_count:,}")
    print(f"–í—Å–µ–≥–æ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π: {total_duplicate_records:,}")
    
    if duplicate_count > 0:
        print("–¢–æ–ø-10 –¥—É–±–ª–µ–π:")
        duplicates.orderBy(desc("count")).show(10)
    
    df_deduplicated = df_partitioned.dropDuplicates(key_fields)
else:
    df_deduplicated = df_with_digits

final_count = df_deduplicated.count()
print(f"–§–∏–Ω–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç: {final_count:,} –∑–∞–ø–∏—Å–µ–π")
print(f"–§–∏–Ω–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä—Ç–∏—Ü–∏–π: {df_deduplicated.rdd.getNumPartitions()}")

df_deduplicated.cache()
```
![](/screen/6.png)

### –ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ –≤—Ä–µ–º–µ–Ω–∏
```python
%pyspark


if "year" in df_deduplicated.columns and "month" in df_deduplicated.columns:
    
    df_time_partitioned = df_deduplicated.repartition("year", "month")
    print(f"–î–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ–ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω—ã –ø–æ –≤—Ä–µ–º–µ–Ω–∏: {df_time_partitioned.rdd.getNumPartitions()} –ø–∞—Ä—Ç–∏—Ü–∏–π")
    
    agg_expressions = []
    
    if "exp_imp" in df_deduplicated.columns:
        agg_expressions.append(countDistinct("exp_imp").alias("unique_exp_imp"))
    
    if "hs9" in df_deduplicated.columns:
        agg_expressions.append(countDistinct("hs9").alias("unique_hs9"))
    
    if "country" in df_deduplicated.columns:
        agg_expressions.append(countDistinct("country").alias("unique_countries"))
    
    numeric_cols = ["customs", "q1", "q2", "value_yen"]
    for col_name in numeric_cols:
        if col_name in df_deduplicated.columns:
            agg_expressions.extend([
                avg(col_name).alias(f"avg_{col_name}"),
                expr(f"percentile_approx({col_name}, 0.5)").alias(f"median_{col_name}")
            ])
    
    agg_expressions.append(count("*").alias("total_records"))
    
    time_aggregation = df_time_partitioned.groupBy("year", "month").agg(*agg_expressions).orderBy("year", "month").cache()
    
    print("–ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ –≤—Ä–µ–º–µ–Ω–∏:")
    time_aggregation.show(50)
    
    print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–≥—Ä–µ–≥–∞—Ü–∏–∏: {time_aggregation.count():,} –∑–∞–ø–∏—Å–µ–π")
    print(f"–ü–∞—Ä—Ç–∏—Ü–∏–π –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ: {time_aggregation.rdd.getNumPartitions()}")
    
else:
    print("–í—Ä–µ–º–µ–Ω–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
    time_aggregation = None
```
![](/screen/7_1.png)
![](/screen/7_2.png)

### –ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —á–∏—Å–ª–æ–≤–æ–π –∫–æ–ª–æ–Ω–∫–∏

```python
%pyspark


numeric_column = "value_yen"

if numeric_column in df_with_metrics.columns:
    
    numeric_data = df_with_metrics.select(numeric_column).filter(col(numeric_column).isNotNull()).cache()
    
    print(f"\n–ê–Ω–∞–ª–∏–∑ –∫–æ–ª–æ–Ω–∫–∏: {numeric_column}")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {numeric_data.count():,}")
    
    # 1. –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ - –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    
    histogram_sample = numeric_data.sample(0.01, seed=42).cache()
    sample_count = histogram_sample.count()
    print(f"\n–†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã: {sample_count:,}")
    
    hist_stats = histogram_sample.agg(
        min(numeric_column).alias("min_val"),
        max(numeric_column).alias("max_val"),
        avg(numeric_column).alias("mean_val"),
        stddev(numeric_column).alias("std_val")
    ).collect()[0]
    
    print(f"–î–∏–∞–ø–∞–∑–æ–Ω: [{hist_stats['min_val']:,}, {hist_stats['max_val']:,}]")
    print(f"–°—Ä–µ–¥–Ω–µ–µ: {hist_stats['mean_val']:,.2f}")
    print(f"–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {hist_stats['std_val']:,.2f}")
    
    hist_data = histogram_sample.rdd.map(lambda row: float(row[0])).histogram(50)
    bins = hist_data[0]
    frequencies = hist_data[1]
    
    print(f"–ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ —Å–æ–∑–¥–∞–Ω–∞: {len(bins)-1} –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤")
    print("–ü–µ—Ä–≤—ã–µ 10 –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤ –∏ —á–∞—Å—Ç–æ—Ç:")
    
    num_to_show = 10
    if len(frequencies) < 10:
        num_to_show = len(frequencies)
    
    for i in range(num_to_show):
        print(f"   [{bins[i]:,.0f}, {bins[i+1]:,.0f}): {frequencies[i]:,}")
    
    # 2. 95% –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª
    
    print(f"\n2. 95% –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –¥–ª—è {numeric_column}:")
    
    full_stats = numeric_data.agg(
        count(numeric_column).alias("n"),
        avg(numeric_column).alias("mean"),
        stddev(numeric_column).alias("stddev")
    ).collect()[0]
    
    n = full_stats["n"]
    mean_val = full_stats["mean"]
    std_val = full_stats["stddev"]
    
    print(f"–ü–æ–ª–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: n={n:,}, mean={mean_val:.2f}, std={std_val:.2f}")
    
    
    print(f"\n–ú–µ—Ç–æ–¥–∏–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞:")
    print(f"–†–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏: n={n:,}")
    
    if n >= 30:
        print("n >= 30")
        print("–ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ—Ä–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å z-–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–º –∑–Ω–∞—á–µ–Ω–∏–µ–º")
        print("–î–ª—è 95%: z = 1.96")
        
        z_critical = 1.96
        margin_error = z_critical * (std_val / math.sqrt(n))
        ci_lower = mean_val - margin_error
        ci_upper = mean_val + margin_error
        
        method = "–¶–ü–¢ + –Ω–æ—Ä–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ"
        
    else:
        print("n < 30 ") 
        print("df = n-1")
        print("4. –î–ª—è 95%: t-–∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ ‚âà 2.0")
        
        t_critical = 2.0
        margin_error = t_critical * (std_val / math.sqrt(n))
        ci_lower = mean_val - margin_error
        ci_upper = mean_val + margin_error
        
        method = "t-—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ"
    
    print(f"\n–†–µ–∑—É–ª—å—Ç–∞—Ç 95% –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞:")
    print(f"   –ú–µ—Ç–æ–¥: {method}")
    print(f"   –ò–Ω—Ç–µ—Ä–≤–∞–ª: [{ci_lower:,.2f}, {ci_upper:,.2f}]")
    print(f"   –®–∏—Ä–∏–Ω–∞ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞: {ci_upper - ci_lower:,.2f}")
    print(f"   –ü–æ–≥—Ä–µ—à–Ω–æ—Å—Ç—å: ¬±{margin_error:,.2f}")
    
    print(f"\n–° –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é 95% –∏—Å—Ç–∏–Ω–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ {numeric_column}")
    print(f"–Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –∏–Ω—Ç–µ—Ä–≤–∞–ª–µ [{ci_lower:,.2f}, {ci_upper:,.2f}]")
```
![](/screen/8_1.png)

![](/screen/8_2.png)

![](/screen/8_3.png)

### –ì—Ä–∞—Ñ–∏–∫ —Å—Ä–µ–¥–Ω–µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –¥–µ–Ω–µ–∂—ç–Ω—ã—Ö –≤–ª–æ–∂–µ–Ω–∏–π –≤ –ô–µ–Ω–∞—Ö (—Ç—ã—Å.) –ø–æ –º–µ—Å—è—Ü–∞–º

```python
if time_aggregation is not None and f"avg_{numeric_column}" in time_aggregation.columns:
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞
    monthly_avg_data = time_aggregation.select("year", "month", f"avg_{numeric_column}", "total_records").orderBy("year", "month")
    
    
    chart_data = monthly_avg_data.withColumn("period", 
                                            concat(col("year").cast("string"), 
                                                  lit("-"), 
                                                  lpad(col("month"), 2, "0")))
    
    # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    rows = chart_data.collect()
    
    print("%table period\taverage_value\ttotal_records")
    for row in rows:
        period = row.period
        avg_val = getattr(row, f"avg_{numeric_column}")
        total = row.total_records
        print(f"{period}\t{avg_val:.2f}\t{total}")
    
    print(f"\n–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(rows)} —Ç–æ—á–µ–∫ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞")

else:
    print("–î–∞–Ω–Ω—ã–µ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞ —Å—Ä–µ–¥–Ω–µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
```
![](/screen/8_4.png)

### Merge –º–µ—Ç—Ä–∏–∫–∏

```python
%pyspark

if time_aggregation is not None:
    df_for_join = df_deduplicated.repartition("year", "month")
    time_agg_for_join = time_aggregation.repartition("year", "month")
    
    print("–î–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ–ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ join")
    
    df_with_metrics = df_for_join.join(time_agg_for_join, ["year", "month"], "left").cache()
    
    print(f"–î–∞—Ç–∞—Å–µ—Ç —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏: {df_with_metrics.count():,} –∑–∞–ø–∏—Å–µ–π")
    print(f"–ü–∞—Ä—Ç–∏—Ü–∏–π –ø–æ—Å–ª–µ join: {df_with_metrics.rdd.getNumPartitions()}")
    
    print("–û–±—Ä–∞–∑–µ—Ü –¥–∞–Ω–Ω—ã—Ö —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏:")
    sample_cols = [col for col in ["year", "month", "value_yen", "unique_countries", "avg_value_yen"] if col in df_with_metrics.columns]
    df_with_metrics.select(*sample_cols).show(10)
    
    null_metrics_count = df_with_metrics.filter(col("total_records").isNull()).count()
    print(f"–ó–∞–ø–∏—Å–µ–π –±–µ–∑ –º–µ—Ç—Ä–∏–∫: {null_metrics_count:,}")
    
else:
    df_with_metrics = df_deduplicated
    print("–ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –±–µ–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫")
```

![](/screen/9.png)

### –°–ª—É—á–∞–π–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–∞ 3 —á–∞—Å—Ç–∏

```python
%pyspark


df_sample1, df_sample2, df_sample3 = df_with_metrics.randomSplit([0.25, 0.25, 0.5], seed=42)

df_sample1.cache()
df_sample2.cache() 
df_sample3.cache()

count1 = df_sample1.count()
count2 = df_sample2.count()
count3 = df_sample3.count()
total = count1 + count2 + count3

print(f"–†–ê–ó–î–ï–õ–ï–ù–ò–ï –î–ê–¢–ê–°–ï–¢–ê:")
print(f"   –í—ã–±–æ—Ä–∫–∞ 1: {count1:,} –∑–∞–ø–∏—Å–µ–π ({count1/total*100:.1f}%)")
print(f"   –í—ã–±–æ—Ä–∫–∞ 2: {count2:,} –∑–∞–ø–∏—Å–µ–π ({count2/total*100:.1f}%)")
print(f"   –í—ã–±–æ—Ä–∫–∞ 3: {count3:,} –∑–∞–ø–∏—Å–µ–π ({count3/total*100:.1f}%)")
print(f"   –í—Å–µ–≥–æ: {total:,} –∑–∞–ø–∏—Å–µ–π")

print(f"–ü–∞—Ä—Ç–∏—Ü–∏–∏ –≤ –≤—ã–±–æ—Ä–∫–∞—Ö:")
print(f"   –í—ã–±–æ—Ä–∫–∞ 1: {df_sample1.rdd.getNumPartitions()} –ø–∞—Ä—Ç–∏—Ü–∏–π")
print(f"   –í—ã–±–æ—Ä–∫–∞ 2: {df_sample2.rdd.getNumPartitions()} –ø–∞—Ä—Ç–∏—Ü–∏–π")
print(f"   –í—ã–±–æ—Ä–∫–∞ 3: {df_sample3.rdd.getNumPartitions()} –ø–∞—Ä—Ç–∏—Ü–∏–π")

print("\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –≥–æ–¥–∞–º –≤ –≤—ã–±–æ—Ä–∫–∞—Ö:")
for i, df_sample in enumerate([df_sample1, df_sample2, df_sample3], 1):
    if "year" in df_sample.columns:
        year_dist = df_sample.groupBy("year").count().orderBy("year")
        print(f"\n–í—ã–±–æ—Ä–∫–∞ {i} - —Ç–æ–ø-10 –≥–æ–¥–æ–≤:")
        year_dist.show(10)
```
![](/screen/10.png)

### –î–æ–ø.–∑–∞–¥–∞–Ω–∏–µ 2

```python
%pyspark

# –î–∞–Ω–Ω—ã–µ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞
competitor_successes = 5
competitor_failures = 995

# –î–∞–Ω–Ω—ã–µ –∫–æ–º–ø–∞–Ω–∏–∏
our_failures = 200

# –ê–ø—Ä–∏–æ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞
alpha_prior = competitor_successes
beta_prior = competitor_failures

# –ê–ø–æ—Å—Ç–µ—Ä–∏–æ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ—Å–ª–µ –Ω–∞—à–∏—Ö –Ω–∞–±–ª—é–¥–µ–Ω–∏–π
alpha_posterior = alpha_prior + 0 
beta_posterior = beta_prior + our_failures

# –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —É—Å–ø–µ—Ö–∞ —Å–ª–µ–¥—É—é—â–µ–≥–æ –ø—Ä–æ—Ç–æ—Ç–∏–ø–∞
probability_success = alpha_posterior / (alpha_posterior + beta_posterior)

print(f"–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —É—Å–ø–µ—Ö–∞ 201-–≥–æ –ø—Ä–æ—Ç–æ—Ç–∏–ø–∞: {probability_success:.4f} –∏–ª–∏ {probability_success*100:.2f}%")

# –î–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª
beta_dist = stats.beta(alpha_posterior, beta_posterior)
ci_lower, ci_upper = beta_dist.interval(0.95)
print(f"95% –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª: [{ci_lower:.4f}, {ci_upper:.4f}]")
```
![](/screen/11.png)



























