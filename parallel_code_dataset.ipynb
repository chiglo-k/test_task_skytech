{
  "metadata": {
    "name": "parallel_code_dataset",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import concat, lpad, lit\nimport pandas as pd\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Проверка кластера\nprint(f\"Spark версия: {spark.version}\")\nprint(f\"Доступно ядер: {spark.sparkContext.defaultParallelism}\")\nprint(f\"Мастер URL: {spark.sparkContext.master}\")\n\n# Информация о кластере\nprint(f\"Executor instances: {spark.conf.get(\u0027spark.executor.instances\u0027, \u0027auto\u0027)}\")\nprint(f\"Executor memory: {spark.conf.get(\u0027spark.executor.memory\u0027, \u0027default\u0027)}\")"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nspark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\nspark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\nspark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\nspark.conf.set(\"spark.sql.adaptive.localShuffleReader.enabled\", \"true\")\nspark.conf.set(\"spark.sql.shuffle.partitions\", \"400\")\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nbucket_name \u003d \u0027sparkwrk\u0027\n\ndf \u003d spark.read.option(\"header\", True).csv(f\u0027s3a://{bucket_name}/bi_datasets/custom_1988_2020.csv\u0027)  # Датасет по трейдингу (Спасибо, Японскому правительству!)"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\ninitial_partitions \u003d df.rdd.getNumPartitions()\n\nprint(f\"Исходное количество партиций: {initial_partitions}\")\n\nprint(\"Оригинальные колонки:\")\nall_columns \u003d df.columns\nfor i, col_name in enumerate(all_columns):\n    print(f\"   {i}: \u0027{col_name}\u0027\")\n\nprint(f\"\\nВсего колонок: {len(all_columns)}\")\n\nrename_dict \u003d {}\nfor col_name in all_columns:\n    if \"198801\" in str(col_name) or col_name \u003d\u003d \"198801\":\n        rename_dict[col_name] \u003d \"ym\"\n    elif col_name \u003d\u003d \"1\":\n        rename_dict[col_name] \u003d \"exp_imp\"  \n    elif col_name \u003d\u003d \"103\":\n        rename_dict[col_name] \u003d \"hs9\"\n    elif col_name \u003d\u003d \"100\":\n        rename_dict[col_name] \u003d \"customs\"\n    elif \"000000190\" in str(col_name) or col_name \u003d\u003d \"000000190\":\n        rename_dict[col_name] \u003d \"country\"\n    elif col_name \u003d\u003d \"0\":\n        rename_dict[col_name] \u003d \"q1\"\n    elif col_name \u003d\u003d \"35843\":\n        rename_dict[col_name] \u003d \"q2\"  \n    elif col_name \u003d\u003d \"34353\":\n        rename_dict[col_name] \u003d \"value_yen\"\n\nprint(f\"\\nСоответствия:\")\n\nfor old_name, new_name in rename_dict.items():\n    print(f\"   \u0027{old_name}\u0027 -\u003e \u0027{new_name}\u0027\")\n\ndf_processed \u003d df.select([col(c).alias(rename_dict.get(c, c)) for c in df.columns])\n\n\nif initial_partitions * 2 \u003e 200:\n    optimal_partitions \u003d initial_partitions * 2\nelse:\n    optimal_partitions \u003d 200\n\ndf_processed \u003d df_processed.repartition(optimal_partitions)\nprint(f\"Оптимизированное количество партиций: {df_processed.rdd.getNumPartitions()}\")\n\n# Приводим типы данных\nif \"ym\" in df_processed.columns:\n    df_processed \u003d df_processed.withColumn(\"ym\", col(\"ym\").cast(IntegerType()))\n    df_processed \u003d df_processed.withColumn(\"year\", (col(\"ym\") / 100).cast(IntegerType()))\n    df_processed \u003d df_processed.withColumn(\"month\", when((col(\"ym\") % 100) \u003c 10, concat(lit(\"0\"), (col(\"ym\") % 100).cast(StringType()))).otherwise((col(\"ym\") % 100).cast(StringType())))\n\nif \"exp_imp\" in df_processed.columns:\n    df_processed \u003d df_processed.withColumn(\"exp_imp\", col(\"exp_imp\").cast(StringType()))\n\nif \"hs9\" in df_processed.columns:\n    df_processed \u003d df_processed.withColumn(\"hs9\", col(\"hs9\").cast(StringType()))\n\nif \"customs\" in df_processed.columns:\n    df_processed \u003d df_processed.withColumn(\"customs\", col(\"customs\").cast(IntegerType()))\n\nif \"country\" in df_processed.columns:\n    df_processed \u003d df_processed.withColumn(\"country\", col(\"country\").cast(StringType()))\n\nif \"q1\" in df_processed.columns:\n    df_processed \u003d df_processed.withColumn(\"q1\", col(\"q1\").cast(LongType()))\n\nif \"q2\" in df_processed.columns:\n    df_processed \u003d df_processed.withColumn(\"q2\", col(\"q2\").cast(LongType()))\n\nif \"value_yen\" in df_processed.columns:\n    df_processed \u003d df_processed.withColumn(\"value_yen\", col(\"value_yen\").cast(LongType()))\n\ndf_processed.cache()\n\nprint(\"Схема после обработки:\")\ndf_processed.printSchema()\n\nprint(\"Образец данных:\")\navailable_cols \u003d [col for col in [\"year\", \"month\", \"exp_imp\", \"hs9\", \"country\", \"q1\", \"q2\", \"value_yen\"] if col in df_processed.columns]\ndf_processed.select(*available_cols).show(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n\nfilters \u003d []\n\nif \"year\" in df_processed.columns and \"month\" in df_processed.columns:\n    filters.append(col(\"year\").isNotNull() \u0026 col(\"month\").isNotNull())\n\nif \"exp_imp\" in df_processed.columns:\n    filters.append(col(\"exp_imp\").isNotNull() \u0026 (col(\"exp_imp\") !\u003d \"\"))\n\nif \"hs9\" in df_processed.columns:\n    filters.append(col(\"hs9\").isNotNull() \u0026 (col(\"hs9\") !\u003d \"\"))\n\nif \"country\" in df_processed.columns:\n    filters.append(col(\"country\").isNotNull() \u0026 (col(\"country\") !\u003d \"\"))\n\nif \"value_yen\" in df_processed.columns:\n    filters.append(col(\"value_yen\").isNotNull() \u0026 (col(\"value_yen\") \u003e 0))\n\nif \"q1\" in df_processed.columns:\n    filters.append(col(\"q1\").isNotNull())\n\nif \"q2\" in df_processed.columns:\n    filters.append(col(\"q2\").isNotNull())\n\n# Объединяем все фильтры\ncombined_filter \u003d filters[0]\nfor filter_condition in filters[1:]:\n    combined_filter \u003d combined_filter \u0026 filter_condition\n\ndf_clean \u003d df_processed.filter(combined_filter)\nprint(f\"После удаления пустых: {df_clean.count():,} записей\")\n\n# Фильтруем строки с цифрами в hs9\nif \"hs9\" in df_clean.columns:\n    df_with_digits \u003d df_clean.filter(col(\"hs9\").rlike(\".*[0-9].*\"))\n    print(f\"После фильтрации строк с цифрами: {df_with_digits.count():,} записей\")\nelse:\n    df_with_digits \u003d df_clean\n    print(\"Колонка hs9 не найдена\")\n\n# Поиск дублей\nkey_fields \u003d [field for field in [\"year\", \"month\", \"exp_imp\", \"hs9\", \"country\", \"customs\"] if field in df_with_digits.columns]\nprint(f\"Ключевые поля для дедупликации: {key_fields}\")\n\nif len(key_fields) \u003e 0:\n    if \"country\" in key_fields:\n        df_partitioned \u003d df_with_digits.repartition(\"country\")\n        print(\"Данные перепартиционированы по country\")\n    else:\n        df_partitioned \u003d df_with_digits\n    \n    print(\"Анализ дублей с параллелизмом...\")\n    duplicates \u003d df_partitioned.groupBy(*key_fields).count().filter(col(\"count\") \u003e 1).cache()\n    \n    duplicate_count \u003d duplicates.count()\n    total_duplicate_records \u003d duplicates.agg(sum(\"count\")).collect()[0][0] if duplicate_count \u003e 0 else 0\n    \n    print(f\"Найдено групп дублей: {duplicate_count:,}\")\n    print(f\"Всего дублированных записей: {total_duplicate_records:,}\")\n    \n    if duplicate_count \u003e 0:\n        print(\"Топ-10 дублей:\")\n        duplicates.orderBy(desc(\"count\")).show(10)\n    \n    df_deduplicated \u003d df_partitioned.dropDuplicates(key_fields)\nelse:\n    df_deduplicated \u003d df_with_digits\n\nfinal_count \u003d df_deduplicated.count()\nprint(f\"Финальный датасет: {final_count:,} записей\")\nprint(f\"Финальное количество партиций: {df_deduplicated.rdd.getNumPartitions()}\")\n\ndf_deduplicated.cache()"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n\nif \"year\" in df_deduplicated.columns and \"month\" in df_deduplicated.columns:\n    \n    df_time_partitioned \u003d df_deduplicated.repartition(\"year\", \"month\")\n    print(f\"Данные перепартиционированы по времени: {df_time_partitioned.rdd.getNumPartitions()} партиций\")\n    \n    agg_expressions \u003d []\n    \n    if \"exp_imp\" in df_deduplicated.columns:\n        agg_expressions.append(countDistinct(\"exp_imp\").alias(\"unique_exp_imp\"))\n    \n    if \"hs9\" in df_deduplicated.columns:\n        agg_expressions.append(countDistinct(\"hs9\").alias(\"unique_hs9\"))\n    \n    if \"country\" in df_deduplicated.columns:\n        agg_expressions.append(countDistinct(\"country\").alias(\"unique_countries\"))\n    \n    numeric_cols \u003d [\"customs\", \"q1\", \"q2\", \"value_yen\"]\n    for col_name in numeric_cols:\n        if col_name in df_deduplicated.columns:\n            agg_expressions.extend([\n                avg(col_name).alias(f\"avg_{col_name}\"),\n                expr(f\"percentile_approx({col_name}, 0.5)\").alias(f\"median_{col_name}\")\n            ])\n    \n    agg_expressions.append(count(\"*\").alias(\"total_records\"))\n    \n    time_aggregation \u003d df_time_partitioned.groupBy(\"year\", \"month\").agg(*agg_expressions).orderBy(\"year\", \"month\").cache()\n    \n    print(\"Агрегация по времени:\")\n    time_aggregation.show(50)\n    \n    print(f\"Результат агрегации: {time_aggregation.count():,} записей\")\n    print(f\"Партиций в результате: {time_aggregation.rdd.getNumPartitions()}\")\n    \nelse:\n    print(\"Временные колонки не найдены\")\n    time_aggregation \u003d None"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n\nnumeric_column \u003d \"value_yen\"\n\nif numeric_column in df_with_metrics.columns:\n    \n    numeric_data \u003d df_with_metrics.select(numeric_column).filter(col(numeric_column).isNotNull()).cache()\n    \n    print(f\"\\nАнализ колонки: {numeric_column}\")\n    print(f\"Количество записей: {numeric_data.count():,}\")\n    \n    # 1. Гистограмма - подготовка данных\n    \n    histogram_sample \u003d numeric_data.sample(0.01, seed\u003d42).cache()\n    sample_count \u003d histogram_sample.count()\n    print(f\"\\nРазмер выборки для гистограммы: {sample_count:,}\")\n    \n    hist_stats \u003d histogram_sample.agg(\n        min(numeric_column).alias(\"min_val\"),\n        max(numeric_column).alias(\"max_val\"),\n        avg(numeric_column).alias(\"mean_val\"),\n        stddev(numeric_column).alias(\"std_val\")\n    ).collect()[0]\n    \n    print(f\"Диапазон: [{hist_stats[\u0027min_val\u0027]:,}, {hist_stats[\u0027max_val\u0027]:,}]\")\n    print(f\"Среднее: {hist_stats[\u0027mean_val\u0027]:,.2f}\")\n    print(f\"Стандартное отклонение: {hist_stats[\u0027std_val\u0027]:,.2f}\")\n    \n    hist_data \u003d histogram_sample.rdd.map(lambda row: float(row[0])).histogram(50)\n    bins \u003d hist_data[0]\n    frequencies \u003d hist_data[1]\n    \n    print(f\"Гистограмма создана: {len(bins)-1} интервалов\")\n    print(\"Первые 10 интервалов и частот:\")\n    \n    num_to_show \u003d 10\n    if len(frequencies) \u003c 10:\n        num_to_show \u003d len(frequencies)\n    \n    for i in range(num_to_show):\n        print(f\"   [{bins[i]:,.0f}, {bins[i+1]:,.0f}): {frequencies[i]:,}\")\n    \n    # 2. 95% доверительный интервал\n    \n    print(f\"\\n2. 95% доверительный интервал для {numeric_column}:\")\n    \n    full_stats \u003d numeric_data.agg(\n        count(numeric_column).alias(\"n\"),\n        avg(numeric_column).alias(\"mean\"),\n        stddev(numeric_column).alias(\"stddev\")\n    ).collect()[0]\n    \n    n \u003d full_stats[\"n\"]\n    mean_val \u003d full_stats[\"mean\"]\n    std_val \u003d full_stats[\"stddev\"]\n    \n    print(f\"Полная выборка: n\u003d{n:,}, mean\u003d{mean_val:.2f}, std\u003d{std_val:.2f}\")\n    \n    \n    print(f\"\\nМетодика расчета доверительного интервала:\")\n    print(f\"Размер выборки: n\u003d{n:,}\")\n    \n    if n \u003e\u003d 30:\n        print(\"n \u003e\u003d 30\")\n        print(\"Используем нормальное распределение с z-критическим значением\")\n        print(\"Для 95%: z \u003d 1.96\")\n        \n        z_critical \u003d 1.96\n        margin_error \u003d z_critical * (std_val / math.sqrt(n))\n        ci_lower \u003d mean_val - margin_error\n        ci_upper \u003d mean_val + margin_error\n        \n        method \u003d \"ЦПТ + нормальное распределение\"\n        \n    else:\n        print(\"n \u003c 30 \") \n        print(\"df \u003d n-1\")\n        print(\"4. Для 95%: t-критическое ≈ 2.0\")\n        \n        t_critical \u003d 2.0\n        margin_error \u003d t_critical * (std_val / math.sqrt(n))\n        ci_lower \u003d mean_val - margin_error\n        ci_upper \u003d mean_val + margin_error\n        \n        method \u003d \"t-распределение\"\n    \n    print(f\"\\nРезультат 95% доверительного интервала:\")\n    print(f\"   Метод: {method}\")\n    print(f\"   Интервал: [{ci_lower:,.2f}, {ci_upper:,.2f}]\")\n    print(f\"   Ширина интервала: {ci_upper - ci_lower:,.2f}\")\n    print(f\"   Погрешность: ±{margin_error:,.2f}\")\n    \n    print(f\"\\nС вероятностью 95% истинное среднее значение {numeric_column}\")\n    print(f\"находится в интервале [{ci_lower:,.2f}, {ci_upper:,.2f}]\")"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n\nif time_aggregation is not None and f\"avg_{numeric_column}\" in time_aggregation.columns:\n    \n    # Подготавливаем данные для графика\n    monthly_avg_data \u003d time_aggregation.select(\"year\", \"month\", f\"avg_{numeric_column}\", \"total_records\").orderBy(\"year\", \"month\")\n    \n    \n    chart_data \u003d monthly_avg_data.withColumn(\"period\", \n                                            concat(col(\"year\").cast(\"string\"), \n                                                  lit(\"-\"), \n                                                  lpad(col(\"month\"), 2, \"0\")))\n    \n    # Собираем данные\n    rows \u003d chart_data.collect()\n    \n    print(\"%table period\\taverage_value\\ttotal_records\")\n    for row in rows:\n        period \u003d row.period\n        avg_val \u003d getattr(row, f\"avg_{numeric_column}\")\n        total \u003d row.total_records\n        print(f\"{period}\\t{avg_val:.2f}\\t{total}\")\n    \n    print(f\"\\nПодготовлено {len(rows)} точек для графика\")\n\nelse:\n    print(\"Данные для графика среднего значения не найдены\")"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nif time_aggregation is not None:\n    df_for_join \u003d df_deduplicated.repartition(\"year\", \"month\")\n    time_agg_for_join \u003d time_aggregation.repartition(\"year\", \"month\")\n    \n    print(\"Данные перепартиционированы для оптимального join\")\n    \n    df_with_metrics \u003d df_for_join.join(time_agg_for_join, [\"year\", \"month\"], \"left\").cache()\n    \n    print(f\"Датасет с метриками: {df_with_metrics.count():,} записей\")\n    print(f\"Партиций после join: {df_with_metrics.rdd.getNumPartitions()}\")\n    \n    print(\"Образец данных с метриками:\")\n    sample_cols \u003d [col for col in [\"year\", \"month\", \"value_yen\", \"unique_countries\", \"avg_value_yen\"] if col in df_with_metrics.columns]\n    df_with_metrics.select(*sample_cols).show(10)\n    \n    null_metrics_count \u003d df_with_metrics.filter(col(\"total_records\").isNull()).count()\n    print(f\"Записей без метрик: {null_metrics_count:,}\")\n    \nelse:\n    df_with_metrics \u003d df_deduplicated\n    print(\"Используем данные без временных метрик\")"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n\ndf_sample1, df_sample2, df_sample3 \u003d df_with_metrics.randomSplit([0.25, 0.25, 0.5], seed\u003d42)\n\ndf_sample1.cache()\ndf_sample2.cache() \ndf_sample3.cache()\n\ncount1 \u003d df_sample1.count()\ncount2 \u003d df_sample2.count()\ncount3 \u003d df_sample3.count()\ntotal \u003d count1 + count2 + count3\n\nprint(f\"РАЗДЕЛЕНИЕ ДАТАСЕТА:\")\nprint(f\"   Выборка 1: {count1:,} записей ({count1/total*100:.1f}%)\")\nprint(f\"   Выборка 2: {count2:,} записей ({count2/total*100:.1f}%)\")\nprint(f\"   Выборка 3: {count3:,} записей ({count3/total*100:.1f}%)\")\nprint(f\"   Всего: {total:,} записей\")\n\nprint(f\"Партиции в выборках:\")\nprint(f\"   Выборка 1: {df_sample1.rdd.getNumPartitions()} партиций\")\nprint(f\"   Выборка 2: {df_sample2.rdd.getNumPartitions()} партиций\")\nprint(f\"   Выборка 3: {df_sample3.rdd.getNumPartitions()} партиций\")\n\nprint(\"\\nРаспределение по годам в выборках:\")\nfor i, df_sample in enumerate([df_sample1, df_sample2, df_sample3], 1):\n    if \"year\" in df_sample.columns:\n        year_dist \u003d df_sample.groupBy(\"year\").count().orderBy(\"year\")\n        print(f\"\\nВыборка {i} - топ-10 годов:\")\n        year_dist.show(10)"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Данные конкурента\ncompetitor_successes \u003d 5\ncompetitor_failures \u003d 995\n\n# Данные компании\nour_failures \u003d 200\n\n# Априорное распределение на основе данных конкурента\nalpha_prior \u003d competitor_successes\nbeta_prior \u003d competitor_failures\n\n# Апостериорное распределение после наших наблюдений\nalpha_posterior \u003d alpha_prior + 0 \nbeta_posterior \u003d beta_prior + our_failures\n\n# Вероятность успеха следующего прототипа\nprobability_success \u003d alpha_posterior / (alpha_posterior + beta_posterior)\n\nprint(f\"Вероятность успеха 201-го прототипа: {probability_success:.4f} или {probability_success*100:.2f}%\")\n\n# Доверительный интервал\nbeta_dist \u003d stats.beta(alpha_posterior, beta_posterior)\nci_lower, ci_upper \u003d beta_dist.interval(0.95)\nprint(f\"95% доверительный интервал: [{ci_lower:.4f}, {ci_upper:.4f}]\")"
    }
  ]
}